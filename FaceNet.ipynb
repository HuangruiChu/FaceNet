{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceNet的实现\n",
    "\n",
    "## 参考 https://github.com/tamerthamoqa/facenet-pytorch-vggface2\n",
    "\n",
    "## 修改部分：\n",
    "\n",
    "1. 修复了变成ipynb后的bug。\n",
    "\n",
    "2. 减少了无用的plot，并改善plot的画质。\n",
    "\n",
    "3. 修正了loss的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本的引入，按字母表排序\n",
    "import argparse\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import os\n",
    "import random\n",
    "#from random import shuffle\n",
    "import sys\n",
    "sys.path.append(r'dataloaders')\n",
    "#torch部分的import\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.distance import PairwiseDistance\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#从py文件中引入functions\n",
    "from dataloaders.LFWDataset import LFWDataset\n",
    "from dataloaders.triplet_loss_dataloader import TripletFaceDataset\n",
    "from validate_on_LFW import evaluate_lfw\n",
    "from plot import plot_roc_lfw, plot_accuracy_lfw, plot_triplet_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设立args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config():\n",
    "    parser = argparse.ArgumentParser(description=\"Training a FaceNet facial recognition model using Triplet Loss.\")\n",
    "    # Dataset\n",
    "    parser.add_argument('--dataroot', '-d', type=str, default=\"VGG\",\n",
    "                        help=\"(REQUIRED) Absolute path to the dataset folder\"\n",
    "                        )\n",
    "    # LFW\n",
    "    parser.add_argument('--lfw', type=str, default=\"Cropped_images\",\n",
    "                        help=\"(REQUIRED) Absolute path to the labeled faces in the wild dataset folder\"\n",
    "                        )\n",
    "    parser.add_argument('--dataset_csv', type=str, default='datasets/lfw_train.csv',\n",
    "                        help=\"Path to the csv file containing the image paths of the training dataset.\"\n",
    "                        )\n",
    "    parser.add_argument('--lfw_batch_size', default=64, type=int,\n",
    "                        help=\"Batch size for LFW dataset (default: 64)\"\n",
    "                        )\n",
    "    parser.add_argument('--lfw_validation_epoch_interval', default=1, type=int,\n",
    "                        help=\"Perform LFW validation every n epoch interval (default: every 1 epoch)\"\n",
    "                        )\n",
    "    # Training settings\n",
    "    parser.add_argument('--model', type=str, default=\"resnet18\", choices=[\"resnet18\",\"resnet34\"],\n",
    "        help=\"The required model architecture for training: (“resnet18”,'resnet34'), (default: 'resnet34')\"\n",
    "                        )\n",
    "    parser.add_argument('--epochs', default=10, type=int,\n",
    "                        help=\"Required training epochs (default: 30)\"\n",
    "                        )\n",
    "    parser.add_argument('--training_triplets_path', default=None, type=str,\n",
    "        help=\"Path to training triplets numpy file in 'datasets/' folder to skip training triplet generation step.\"\n",
    "                        )\n",
    "    parser.add_argument('--num_triplets_train', default=10000, type=int,\n",
    "                        help=\"Number of triplets for training (default: 1100000)\"\n",
    "                        )\n",
    "    parser.add_argument('--resume_path', default='',  type=str,\n",
    "        help='path to latest model checkpoint: (Model_training_checkpoints/model_resnet34_epoch_0.pt file) (default: None)'\n",
    "                        )\n",
    "    parser.add_argument('--batch_size', default=64, type=int,\n",
    "                        help=\"Batch size (default: 64)\"\n",
    "                        )\n",
    "    parser.add_argument('--num_workers', default=0, type=int,\n",
    "                        help=\"Number of workers for data loaders (default: 0)\"\n",
    "                        #不为零就会报错，原因是shared memory不够，而sharedmemory提高需要sudo权限。据我搜集的信息，目前没有其他解决办法\n",
    "                        )\n",
    "    parser.add_argument('--embedding_dim', default=256, type=int,\n",
    "                        help=\"Dimension of the embedding vector (default: 128)\"\n",
    "                        )\n",
    "    parser.add_argument('--pretrained', default=True, type=bool,\n",
    "                        help=\"Download a model pretrained on the ImageNet dataset (Default: False)\"\n",
    "                        )\n",
    "    parser.add_argument('--optimizer', type=str, default=\"adam\", choices=[\"sgd\", \"adagrad\", \"rmsprop\", \"adam\"],\n",
    "        help=\"Required optimizer for training the model: ('sgd','adagrad','rmsprop','adam'), (default: 'adam')\"\n",
    "                        )\n",
    "    parser.add_argument('--lr', default=0.1, type=float,\n",
    "                        help=\"Learning rate for the optimizer (default: 0.1)\"\n",
    "                        )\n",
    "    parser.add_argument('--margin', default=0.5, type=float,\n",
    "                        help='margin for triplet loss (default: 0.5)'\n",
    "                        )\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "args=config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些辅助functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_architecture(model_architecture, pretrained, embedding_dimension):\n",
    "    if model_architecture == \"resnet18\":\n",
    "        model = Resnet18Triplet(\n",
    "            embedding_dimension=embedding_dimension,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "    elif model_architecture == \"resnet34\":\n",
    "        model = Resnet34Triplet(\n",
    "            embedding_dimension=embedding_dimension,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "    print(\"Using {} model architecture.\".format(model_architecture))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_gpu_mode(model):\n",
    "    flag_train_gpu = torch.cuda.is_available()\n",
    "    flag_train_multi_gpu = False\n",
    "\n",
    "    if flag_train_gpu and torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        model.cuda()\n",
    "        flag_train_multi_gpu = True\n",
    "        print('Using multi-gpu training.')\n",
    "\n",
    "    elif flag_train_gpu and torch.cuda.device_count() == 1:\n",
    "        model.cuda()\n",
    "        print('Using single-gpu training.')\n",
    "    return model, flag_train_multi_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimizer(optimizer, model, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer_model = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    elif optimizer == \"adagrad\":\n",
    "        optimizer_model = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        optimizer_model = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer_model = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, amsgrad=True)\n",
    "\n",
    "    return optimizer_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18Triplet(nn.Module):\n",
    "    \"\"\"Constructs a ResNet-18 model for FaceNet training using triplet loss.\n",
    "    Args:\n",
    "        embedding_dimension (int): Required dimension of the resulting embedding layer that is outputted by the model.\n",
    "                                   using triplet loss. Defaults to 128.\n",
    "        pretrained (bool): If True, returns a model pre-trained on the ImageNet dataset from a PyTorch repository.\n",
    "                           Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension=128, pretrained=False):\n",
    "        super(Resnet18Triplet, self).__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=pretrained)\n",
    "        input_features_fc_layer = self.model.fc.in_features\n",
    "        # Output embedding\n",
    "        self.model.fc = nn.Linear(input_features_fc_layer, embedding_dimension)\n",
    "\n",
    "    def l2_norm(self, input):\n",
    "        \"\"\"Perform l2 normalization operation on an input vector.\n",
    "        code copied from liorshk's repository: https://github.com/liorshk/facenet_pytorch/blob/master/model.py\n",
    "        \"\"\"\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "        output = _output.view(input_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Forward pass to output the embedding vector (feature vector) after l2-normalization and multiplication\n",
    "        by scalar (alpha).\"\"\"\n",
    "        embedding = self.model(images)\n",
    "        embedding = self.l2_norm(embedding)\n",
    "        # Multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf\n",
    "        #   Equation 9: number of classes in VGGFace2 dataset = 9131\n",
    "        #   lower bound on alpha = 5, multiply alpha by 2; alpha = 10\n",
    "        alpha = 10\n",
    "        embedding = embedding * alpha\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet34Triplet(nn.Module):\n",
    "    \"\"\"Constructs a ResNet-34 model for FaceNet training using triplet loss.\n",
    "    Args:\n",
    "        embedding_dimension (int): Required dimension of the resulting embedding layer that is outputted by the model.\n",
    "                                   using triplet loss. Defaults to 128.\n",
    "        pretrained (bool): If True, returns a model pre-trained on the ImageNet dataset from a PyTorch repository.\n",
    "                           Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension=128, pretrained=False):\n",
    "        super(Resnet34Triplet, self).__init__()\n",
    "        #从pytorch官方下载resnet34\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet34', pretrained=pretrained)\n",
    "        input_features_fc_layer = self.model.fc.in_features\n",
    "        # Output embedding\n",
    "        self.model.fc = nn.Linear(input_features_fc_layer, embedding_dimension)\n",
    "\n",
    "    def l2_norm(self, input):\n",
    "        \"\"\"Perform l2 normalization operation on an input vector.\n",
    "        code copied from liorshk's repository: https://github.com/liorshk/facenet_pytorch/blob/master/model.py\n",
    "        \"\"\"\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "        output = _output.view(input_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Forward pass to output the embedding vector (feature vector) after l2-normalization and multiplication\n",
    "        by scalar (alpha).\"\"\"\n",
    "        embedding = self.model(images)\n",
    "        embedding = self.l2_norm(embedding)\n",
    "        # Multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf\n",
    "        #   Equation 9: number of classes in VGGFace2 dataset = 9131\n",
    "        #   lower bound on alpha = 5, multiply alpha by 2; alpha = 10\n",
    "        alpha = 10\n",
    "        embedding = embedding * alpha\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以利用已经训练好的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('model_resnet34_triplet.pt')\n",
    "# model = Resnet34Triplet(embedding_dimension=checkpoint['embedding_dimension'])\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# best_distance_threshold = checkpoint['best_distance_threshold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TripletLoss\n",
    "\n",
    "## 这里只是写出了，后面训练的时候其实使用的是pytorch官方提供的tripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(Function):\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pdist = PairwiseDistance(2)\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = self.pdist.forward(anchor, positive)\n",
    "        neg_dist = self.pdist.forward(anchor, negative)\n",
    "\n",
    "        hinge_dist = torch.clamp(self.margin + pos_dist - neg_dist, min=0.0)\n",
    "        loss = torch.mean(hinge_dist)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triplet(start_epoch, end_epoch, epochs, train_dataloader, lfw_dataloader, lfw_validation_epoch_interval,\n",
    "                  model, model_architecture, optimizer_model, embedding_dimension, batch_size, margin,\n",
    "                  flag_train_multi_gpu):\n",
    "\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        triplet_loss_sum=[]\n",
    "        flag_validate_lfw = (epoch + 1) % lfw_validation_epoch_interval == 0 or (epoch + 1) % epochs == 0\n",
    "        num_valid_training_triplets = 0\n",
    "        l2_distance = PairwiseDistance(2).cuda()\n",
    "        distances, labels = [], []\n",
    "        # Training pass\n",
    "        model.train()\n",
    "        for batch_idx, (batch_sample) in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "            anc_img = batch_sample['anc_img'].cuda()\n",
    "            pos_img = batch_sample['pos_img'].cuda()\n",
    "            neg_img = batch_sample['neg_img'].cuda()\n",
    "\n",
    "            # Forward pass - compute embeddings\n",
    "            anc_embedding, pos_embedding, neg_embedding = model(anc_img), model(pos_img), model(neg_img)\n",
    "\n",
    "            # Forward pass - choose hard negatives only for training\n",
    "            pos_dist = l2_distance.forward(anc_embedding, pos_embedding)\n",
    "            neg_dist = l2_distance.forward(anc_embedding, neg_embedding)\n",
    "\n",
    "            all = (neg_dist - pos_dist < margin).cpu().numpy().flatten()\n",
    "\n",
    "            hard_triplets = np.where(all == 1)\n",
    "            if len(hard_triplets[0]) == 0:\n",
    "                continue\n",
    "\n",
    "            anc_hard_embedding = anc_embedding[hard_triplets].cuda()\n",
    "            pos_hard_embedding = pos_embedding[hard_triplets].cuda()\n",
    "            neg_hard_embedding = neg_embedding[hard_triplets].cuda()\n",
    "\n",
    "            # Calculate triplet loss\n",
    "            triplet_loss = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "            output= triplet_loss(\n",
    "                anchor=anc_hard_embedding,\n",
    "                positive=pos_hard_embedding,\n",
    "                negative=neg_hard_embedding\n",
    "            ).cuda()\n",
    "\n",
    "            # Calculating loss\n",
    "            triplet_loss_sum.append(output.item())\n",
    "            num_valid_training_triplets += len(anc_hard_embedding)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_model.zero_grad()\n",
    "            output.backward()\n",
    "            optimizer_model.step()\n",
    "            \n",
    "            #检测accuracy\n",
    "            dists = l2_distance(anc_embedding,neg_embedding)\n",
    "            distances.append(dists.data.cpu().numpy())\n",
    "            labels.append(np.zeros(dists.size(0)))\n",
    "\n",
    "            dists = l2_distance(anc_embedding,pos_embedding)\n",
    "            distances.append(dists.data.cpu().numpy())\n",
    "            labels.append(np.ones(dists.size(0)))\n",
    "\n",
    "        labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "        distances = np.array([subdist for distance in distances for subdist in distance])\n",
    "\n",
    "        true_positive_rate, false_positive_rate, precision, recall, accuracy, roc_auc, best_distances, \\\n",
    "        tar, far = evaluate_lfw(\n",
    "            distances=distances,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        # Model only trains on hard negative triplets\n",
    "        avg_triplet_loss = 0 if (num_valid_training_triplets == 0) else np.mean(triplet_loss_sum)\n",
    "\n",
    "        # Print training statistics and add to log\n",
    "        print(num_valid_training_triplets)\n",
    "        print('Epoch {}:\\tAverage Triplet Loss: {:.4f}\\t Training Accuracy: {:.4f}+-{:.4f}'.format(\n",
    "                epoch + 1,\n",
    "                avg_triplet_loss,\n",
    "                np.mean(accuracy),\n",
    "                np.std(accuracy)\n",
    "            )\n",
    "        ) \n",
    "        print(\"    Number of valid training triplets in epoch: {}\".format(num_valid_training_triplets))\n",
    "        with open('logs/{}_log_triplet.txt'.format(model_architecture), 'a') as f:\n",
    "            val_list = [\n",
    "                epoch + 1,\n",
    "                avg_triplet_loss,\n",
    "                num_valid_training_triplets\n",
    "            ]\n",
    "            log = '\\t'.join(str(value) for value in val_list)\n",
    "            f.writelines(log + '\\n')\n",
    "\n",
    "        # Evaluation pass on LFW dataset\n",
    "        if flag_validate_lfw:\n",
    "            best_distances = validate_lfw(\n",
    "                model=model,\n",
    "                lfw_dataloader=lfw_dataloader,\n",
    "                model_architecture=model_architecture,\n",
    "                epoch=epoch,\n",
    "                epochs=epochs\n",
    "            )\n",
    "\n",
    "        # Save model checkpoint\n",
    "        state = {\n",
    "            'epoch': epoch + 1,\n",
    "            'embedding_dimension': embedding_dimension,\n",
    "            'batch_size_training': batch_size,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_architecture': model_architecture,\n",
    "            'optimizer_model_state_dict': optimizer_model.state_dict()\n",
    "        }\n",
    "\n",
    "        # For storing data parallel model's state dictionary without 'module' parameter\n",
    "        if flag_train_multi_gpu:\n",
    "            state['model_state_dict'] = model.module.state_dict()\n",
    "\n",
    "        # For storing best euclidean distance threshold during LFW validation\n",
    "        if flag_validate_lfw:\n",
    "            state['best_distance_threshold'] = np.mean(best_distances)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        torch.save(state, 'Model_training_checkpoints/model_{}_triplet_epoch_{}.pt'.format(\n",
    "                model_architecture,\n",
    "                epoch + 1\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在LFW数据集上validate测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lfw(model, lfw_dataloader, model_architecture, epoch, epochs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        l2_distance = PairwiseDistance(2).cuda()\n",
    "        distances, labels = [], []\n",
    "\n",
    "        print(\"Validating on LFW! ...\")\n",
    "        progress_bar = enumerate(tqdm(lfw_dataloader))\n",
    "\n",
    "        for batch_index, (data_a, data_b, label) in progress_bar:\n",
    "            data_a, data_b, label = data_a.cuda(), data_b.cuda(), label.cuda()\n",
    "\n",
    "            output_a, output_b = model(data_a), model(data_b)\n",
    "            distance = l2_distance.forward(output_a, output_b)  # Euclidean distance\n",
    "\n",
    "            distances.append(distance.cpu().detach().numpy())\n",
    "            labels.append(label.cpu().detach().numpy())\n",
    "\n",
    "        labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "        distances = np.array([subdist for distance in distances for subdist in distance])\n",
    "\n",
    "        true_positive_rate, false_positive_rate, precision, recall, accuracy, roc_auc, best_distances, \\\n",
    "        tar, far = evaluate_lfw(\n",
    "            distances=distances,\n",
    "            labels=labels\n",
    "        )\n",
    "        # Print statistics and add to log\n",
    "        print(\"Accuracy on LFW: {:.4f}+-{:.4f}\\tPrecision {:.4f}+-{:.4f}\\tRecall {:.4f}+-{:.4f}\\t\"\n",
    "              \"ROC Area Under Curve: {:.4f}\\tBest distance threshold: {:.2f}+-{:.2f}\\t\"\n",
    "              \"TAR: {:.4f}+-{:.4f} @ FAR: {:.4f}\".format(\n",
    "                np.mean(accuracy),\n",
    "                np.std(accuracy),\n",
    "                np.mean(precision),\n",
    "                np.std(precision),\n",
    "                np.mean(recall),\n",
    "                np.std(recall),\n",
    "                roc_auc,\n",
    "                np.mean(best_distances),\n",
    "                np.std(best_distances),\n",
    "                np.mean(tar),\n",
    "                np.std(tar),\n",
    "                np.mean(far)\n",
    "            )\n",
    "        )\n",
    "        with open('logs/lfw_{}_log_triplet.txt'.format(model_architecture), 'a') as f:\n",
    "            val_list = [\n",
    "                epoch + 1,\n",
    "                np.mean(accuracy),\n",
    "                np.std(accuracy),\n",
    "                np.mean(precision),\n",
    "                np.std(precision),\n",
    "                np.mean(recall),\n",
    "                np.std(recall),\n",
    "                roc_auc,\n",
    "                np.mean(best_distances),\n",
    "                np.std(best_distances),\n",
    "                np.mean(tar)\n",
    "            ]\n",
    "            log = '\\t'.join(str(value) for value in val_list)\n",
    "            f.writelines(log + '\\n')\n",
    "    try:\n",
    "        # Plot ROC curve\n",
    "        plot_roc_lfw(\n",
    "            false_positive_rate=false_positive_rate,\n",
    "            true_positive_rate=true_positive_rate,\n",
    "            figure_name=\"plots/roc_plots/roc_{}_epoch_{}_triplet.png\".format(model_architecture, epoch + 1)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return best_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设立dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = args.dataroot\n",
    "lfw_dataroot = args.lfw\n",
    "dataset_csv = args.dataset_csv\n",
    "lfw_batch_size = args.lfw_batch_size\n",
    "lfw_validation_epoch_interval = args.lfw_validation_epoch_interval\n",
    "num_triplets_train = args.num_triplets_train\n",
    "model_architecture = args.model\n",
    "epochs = args.epochs\n",
    "embedding_dimension = args.embedding_dim\n",
    "pretrained = args.pretrained\n",
    "optimizer = args.optimizer\n",
    "learning_rate = args.lr\n",
    "margin = args.margin\n",
    "start_epoch = 0\n",
    "resume_path = args.resume_path\n",
    "\n",
    "# training_triplets_path = args.training_triplets_path\n",
    "#我已经生成了training_triplets_1100000.npy\n",
    "training_triplets_path =\"datasets/training_triplets_10000.npy\"\n",
    "\n",
    "batch_size = args.batch_size\n",
    "num_workers = args.num_workers\n",
    "num_triplets_train = args.num_triplets_train\n",
    "\n",
    "\n",
    "# Define image data pre-processing transforms\n",
    "#   ToTensor() normalizes pixel values between [0, 1]\n",
    "#   Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) normalizes pixel values between [-1, 1]\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize([160, 160]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "# Size 160x160 RGB image\n",
    "lfw_transforms = transforms.Compose([\n",
    "    transforms.Resize([160,160]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "#Set dataloaders\n",
    "lfw_train_dataset=TripletFaceDataset(\n",
    "        root_dir=dataroot,\n",
    "        csv_name=dataset_csv,\n",
    "        num_triplets=num_triplets_train,\n",
    "        training_triplets_path=training_triplets_path,\n",
    "        transform=data_transforms\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    lfw_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "lfw_valid_dataset=LFWDataset(\n",
    "        dir=lfw_dataroot,\n",
    "        pairs_path='datasets/LFW_pairs.txt',\n",
    "        #pairs_path='txt/pairsDevTest.txt',\n",
    "        transform=lfw_transforms\n",
    "    )\n",
    "lfw_dataloader = DataLoader(\n",
    "    lfw_valid_dataset,\n",
    "    batch_size=lfw_batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化model，设定 optimizer，learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = set_model_architecture(\n",
    "    model_architecture=model_architecture,\n",
    "    pretrained=pretrained,\n",
    "    embedding_dimension=embedding_dimension\n",
    ")\n",
    "\n",
    "# # 利用pretrain的model\n",
    "\n",
    "# checkpoint = torch.load('model_resnet34_triplet.pt')\n",
    "# model = Resnet34Triplet(embedding_dimension=checkpoint['embedding_dimension'])\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# best_distance_threshold = checkpoint['best_distance_threshold']\n",
    "\n",
    "\n",
    "\n",
    "# Load model to GPU or multiple GPUs if available\n",
    "model, flag_train_multi_gpu = set_model_gpu_mode(model)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer_model = set_optimizer(\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Resume from a model checkpoint\n",
    "if resume_path:\n",
    "    if os.path.isfile(resume_path):\n",
    "        print(\"Loading checkpoint {} ...\".format(resume_path))\n",
    "\n",
    "        checkpoint = torch.load(resume_path)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "\n",
    "        # In order to load state dict for optimizers correctly, model has to be loaded to gpu first\n",
    "        if flag_train_multi_gpu:\n",
    "            model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        optimizer_model.load_state_dict(checkpoint['optimizer_model_state_dict'])\n",
    "\n",
    "        print(\"Checkpoint loaded: start epoch from checkpoint = {}\\nRunning for {} epochs.\\n\".format(\n",
    "                start_epoch,\n",
    "                epochs - start_epoch\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"WARNING: No checkpoint found at {}!\\nTraining from scratch.\".format(resume_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training loop\n",
    "print(\"Training using triplet loss on {} triplets starting for {} epochs:\\n\".format(\n",
    "        num_triplets_train,\n",
    "        epochs - start_epoch\n",
    "    )\n",
    ")\n",
    "\n",
    "start_epoch = start_epoch\n",
    "end_epoch = start_epoch + epochs\n",
    "# Start training model using Triplet Loss\n",
    "train_triplet(\n",
    "    start_epoch=start_epoch,\n",
    "    end_epoch=end_epoch,\n",
    "    epochs=epochs,\n",
    "    train_dataloader=train_dataloader,\n",
    "    lfw_dataloader=lfw_dataloader,\n",
    "    lfw_validation_epoch_interval=lfw_validation_epoch_interval,\n",
    "    model=model,\n",
    "    model_architecture=model_architecture,\n",
    "    optimizer_model=optimizer_model,\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    batch_size=batch_size,\n",
    "    margin=margin,\n",
    "    flag_train_multi_gpu=flag_train_multi_gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化训练结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triple Loss\n",
    "\n",
    "### Triple loss走高的原因可能是hard_anchor 数量减小了？？？？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Triplet losses plot\n",
    "plot_triplet_losses(\n",
    "    log_dir=\"logs/{}_log_triplet.txt\".format(model_architecture),\n",
    "    epochs=epochs,\n",
    "    figure_name=\"plots/triplet_losses_{}.png\".format(model_architecture)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化 我的model 在 LFW accuracy的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LFW accuracies plot\n",
    "plot_accuracy_lfw(\n",
    "    log_dir=\"logs/lfw_{}_log_triplet.txt\".format(model_architecture),\n",
    "    epochs=epochs,\n",
    "    figure_name=\"plots/lfw_accuracies_{}_triplet.png\".format(model_architecture)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
